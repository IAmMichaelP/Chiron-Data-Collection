{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOrNq2C8CMlg",
        "outputId": "1c27ae78-89fa-470f-86cb-82a15b64c2da"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori, association_rules\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVDlac75Ciw3",
        "outputId": "da1b58d6-dbff-4da4-fe9c-4d4f4de8ae93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                link  \\\n",
            "0  https://www.infowars.com/posts/the-wall-is-rea...   \n",
            "1  https://www.infowars.com/posts/post-vaccinatio...   \n",
            "2  https://www.infowars.com/posts/half-of-all-wom...   \n",
            "3  https://www.infowars.com/posts/high-levels-of-...   \n",
            "4  https://www.infowars.com/posts/epidemiologist-...   \n",
            "\n",
            "                                               title  annotation  \\\n",
            "0  The Wall Is Real: Half of All Thirty-Something...           0   \n",
            "1  “Post-Vaccination Syndrome:” New Paper Identif...           0   \n",
            "2  Half of All Women in US Aged 30-35 Experiencin...           0   \n",
            "3  High Levels of Microplastics Found in Lungs of...           0   \n",
            "4  Epidemiologist Reveals New Data Linking Covid ...           0   \n",
            "\n",
            "                                             content  \n",
            "0  It’s a staple of the so-called “manosphere”—th...  \n",
            "1  A new condition called “post-vaccination syndr...  \n",
            "2  Half of all women in their 30s are now reporti...  \n",
            "3  The lungs of birds contain significant quantit...  \n",
            "4  Nicolas Hulscher, an epidemiologist with the M...  \n"
          ]
        }
      ],
      "source": [
        "data = r\"C:\\Users\\agaro\\Documents\\GitHub\\Chiron\\algorithm\\dataset1.csv\"\n",
        "\n",
        "df = pd.read_csv(data)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj-W3bm3EEqI",
        "outputId": "bb95f929-7914-4a5c-8e79-092949cba4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Indices with missing or empty content: Index([58, 185, 203, 210, 274, 289, 2018, 2019, 2020, 2021], dtype='int64')\n",
            "First file rows: 2801\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation\n",
        "\n",
        "# Checks the contents if there are empty or missing values\n",
        "\n",
        "uncleanContents = df[df['content'].isna() | (df['content'].str.strip() == '')].index\n",
        "\n",
        "# Print the missing/empty indices\n",
        "print(f'Indices with missing or empty content: {uncleanContents}')\n",
        "# print('empty content example: ' + (df['content'])[217])\n",
        "# Fill NaN values with empty strings\n",
        "df['content'].fillna('', inplace=True)\n",
        "\n",
        "print(f\"First file rows: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xNEC6tMGI8q"
      },
      "source": [
        "First step we are doing is data cleaning and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "QyopONrIGPNK",
        "outputId": "87950019-5dcf-4290-bbd1-34b18af4af8d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>annotation</th>\n",
              "      <th>content</th>\n",
              "      <th>clean_msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.infowars.com/posts/the-wall-is-rea...</td>\n",
              "      <td>The Wall Is Real: Half of All Thirty-Something...</td>\n",
              "      <td>0</td>\n",
              "      <td>It’s a staple of the so-called “manosphere”—th...</td>\n",
              "      <td>Its a staple of the socalled manospherethat gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.infowars.com/posts/post-vaccinatio...</td>\n",
              "      <td>“Post-Vaccination Syndrome:” New Paper Identif...</td>\n",
              "      <td>0</td>\n",
              "      <td>A new condition called “post-vaccination syndr...</td>\n",
              "      <td>A new condition called postvaccination syndrom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.infowars.com/posts/half-of-all-wom...</td>\n",
              "      <td>Half of All Women in US Aged 30-35 Experiencin...</td>\n",
              "      <td>0</td>\n",
              "      <td>Half of all women in their 30s are now reporti...</td>\n",
              "      <td>Half of all women in their 30s are now reporti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.infowars.com/posts/high-levels-of-...</td>\n",
              "      <td>High Levels of Microplastics Found in Lungs of...</td>\n",
              "      <td>0</td>\n",
              "      <td>The lungs of birds contain significant quantit...</td>\n",
              "      <td>The lungs of birds contain significant quantit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.infowars.com/posts/epidemiologist-...</td>\n",
              "      <td>Epidemiologist Reveals New Data Linking Covid ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Nicolas Hulscher, an epidemiologist with the M...</td>\n",
              "      <td>Nicolas Hulscher an epidemiologist with the Mc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  \\\n",
              "0  https://www.infowars.com/posts/the-wall-is-rea...   \n",
              "1  https://www.infowars.com/posts/post-vaccinatio...   \n",
              "2  https://www.infowars.com/posts/half-of-all-wom...   \n",
              "3  https://www.infowars.com/posts/high-levels-of-...   \n",
              "4  https://www.infowars.com/posts/epidemiologist-...   \n",
              "\n",
              "                                               title  annotation  \\\n",
              "0  The Wall Is Real: Half of All Thirty-Something...           0   \n",
              "1  “Post-Vaccination Syndrome:” New Paper Identif...           0   \n",
              "2  Half of All Women in US Aged 30-35 Experiencin...           0   \n",
              "3  High Levels of Microplastics Found in Lungs of...           0   \n",
              "4  Epidemiologist Reveals New Data Linking Covid ...           0   \n",
              "\n",
              "                                             content  \\\n",
              "0  It’s a staple of the so-called “manosphere”—th...   \n",
              "1  A new condition called “post-vaccination syndr...   \n",
              "2  Half of all women in their 30s are now reporti...   \n",
              "3  The lungs of birds contain significant quantit...   \n",
              "4  Nicolas Hulscher, an epidemiologist with the M...   \n",
              "\n",
              "                                           clean_msg  \n",
              "0  Its a staple of the socalled manospherethat gl...  \n",
              "1  A new condition called postvaccination syndrom...  \n",
              "2  Half of all women in their 30s are now reporti...  \n",
              "3  The lungs of birds contain significant quantit...  \n",
              "4  Nicolas Hulscher an epidemiologist with the Mc...  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation and i not in [\"'\", '\"', '—', '“', '”', '’', '––', '–']])\n",
        "    return punctuationfree\n",
        "#storing the puntuation free text\n",
        "df['clean_msg']= df['content'].apply(lambda x:remove_punctuation(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "0J7b41LTGFem",
        "outputId": "104a32b5-cc8e-401b-8ac2-f9c03c5b71fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>link</th>\n",
              "      <th>title</th>\n",
              "      <th>annotation</th>\n",
              "      <th>content</th>\n",
              "      <th>clean_msg</th>\n",
              "      <th>msg_lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.infowars.com/posts/the-wall-is-rea...</td>\n",
              "      <td>The Wall Is Real: Half of All Thirty-Something...</td>\n",
              "      <td>0</td>\n",
              "      <td>It’s a staple of the so-called “manosphere”—th...</td>\n",
              "      <td>Its a staple of the socalled manospherethat gl...</td>\n",
              "      <td>its a staple of the socalled manospherethat gl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.infowars.com/posts/post-vaccinatio...</td>\n",
              "      <td>“Post-Vaccination Syndrome:” New Paper Identif...</td>\n",
              "      <td>0</td>\n",
              "      <td>A new condition called “post-vaccination syndr...</td>\n",
              "      <td>A new condition called postvaccination syndrom...</td>\n",
              "      <td>a new condition called postvaccination syndrom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://www.infowars.com/posts/half-of-all-wom...</td>\n",
              "      <td>Half of All Women in US Aged 30-35 Experiencin...</td>\n",
              "      <td>0</td>\n",
              "      <td>Half of all women in their 30s are now reporti...</td>\n",
              "      <td>Half of all women in their 30s are now reporti...</td>\n",
              "      <td>half of all women in their 30s are now reporti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://www.infowars.com/posts/high-levels-of-...</td>\n",
              "      <td>High Levels of Microplastics Found in Lungs of...</td>\n",
              "      <td>0</td>\n",
              "      <td>The lungs of birds contain significant quantit...</td>\n",
              "      <td>The lungs of birds contain significant quantit...</td>\n",
              "      <td>the lungs of birds contain significant quantit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.infowars.com/posts/epidemiologist-...</td>\n",
              "      <td>Epidemiologist Reveals New Data Linking Covid ...</td>\n",
              "      <td>0</td>\n",
              "      <td>Nicolas Hulscher, an epidemiologist with the M...</td>\n",
              "      <td>Nicolas Hulscher an epidemiologist with the Mc...</td>\n",
              "      <td>nicolas hulscher an epidemiologist with the mc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                link  \\\n",
              "0  https://www.infowars.com/posts/the-wall-is-rea...   \n",
              "1  https://www.infowars.com/posts/post-vaccinatio...   \n",
              "2  https://www.infowars.com/posts/half-of-all-wom...   \n",
              "3  https://www.infowars.com/posts/high-levels-of-...   \n",
              "4  https://www.infowars.com/posts/epidemiologist-...   \n",
              "\n",
              "                                               title  annotation  \\\n",
              "0  The Wall Is Real: Half of All Thirty-Something...           0   \n",
              "1  “Post-Vaccination Syndrome:” New Paper Identif...           0   \n",
              "2  Half of All Women in US Aged 30-35 Experiencin...           0   \n",
              "3  High Levels of Microplastics Found in Lungs of...           0   \n",
              "4  Epidemiologist Reveals New Data Linking Covid ...           0   \n",
              "\n",
              "                                             content  \\\n",
              "0  It’s a staple of the so-called “manosphere”—th...   \n",
              "1  A new condition called “post-vaccination syndr...   \n",
              "2  Half of all women in their 30s are now reporti...   \n",
              "3  The lungs of birds contain significant quantit...   \n",
              "4  Nicolas Hulscher, an epidemiologist with the M...   \n",
              "\n",
              "                                           clean_msg  \\\n",
              "0  Its a staple of the socalled manospherethat gl...   \n",
              "1  A new condition called postvaccination syndrom...   \n",
              "2  Half of all women in their 30s are now reporti...   \n",
              "3  The lungs of birds contain significant quantit...   \n",
              "4  Nicolas Hulscher an epidemiologist with the Mc...   \n",
              "\n",
              "                                           msg_lower  \n",
              "0  its a staple of the socalled manospherethat gl...  \n",
              "1  a new condition called postvaccination syndrom...  \n",
              "2  half of all women in their 30s are now reporti...  \n",
              "3  the lungs of birds contain significant quantit...  \n",
              "4  nicolas hulscher an epidemiologist with the mc...  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# standardization: making all letters into lowercase\n",
        "df['msg_lower']= df['clean_msg'].apply(lambda x: x.lower())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oEikEsgcuEa",
        "outputId": "afaf0e8a-81f1-4098-d521-f1c41c33dbf3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGS7SbZ3YP2A",
        "outputId": "37339f09-274e-4460-d85c-d0692e09ed7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'new', 'condition', 'called', 'postvaccination', 'syndrome', 'has', 'been', 'acknowledged', 'by', 'yale', 'scientists', 'investigating', 'the', 'longterm', 'aftereffects', 'of', 'covid19', 'vaccination', 'the', 'condition', 'is', 'characterised', 'by', 'a', 'variety', 'of', 'troubling', 'symptoms', 'including', 'brain', 'fog', 'insomnia', 'tinnitus', 'and', 'heart', 'palpitations', 'and', 'resembles', 'the', 'condition', 'dubbed', 'long', 'covid', 'the', 'symptoms', 'typically', 'develop', 'a', 'day', 'or', 'two', 'after', 'vaccination', 'and', 'can', 'persist', 'for', 'months', 'or', 'even', 'years', 'worsening', 'over', 'time', 'its', 'clear', 'that', 'some', 'individuals', 'are', 'experiencing', 'significant', 'challenges', 'after', 'vaccination', 'our', 'responsibility', 'as', 'scientists', 'and', 'clinicians', 'is', 'to', 'listen', 'to', 'their', 'experiences', 'rigorously', 'investigate', 'the', 'underlying', 'causes', 'and', 'seek', 'ways', 'to', 'help', 'said', 'cosenior', 'author', 'harlan', 'krumholz', 'the', 'researchers', 'looked', 'at', 'blood', 'samples', 'from', '42', 'patients', 'who', 'experienced', 'symptoms', 'of', 'postvaccination', 'syndrome', 'and', 'from', '22', 'who', 'did', 'not', 'those', 'with', 'the', 'syndrome', 'were', 'found', 'to', 'have', 'lower', 'levels', 'of', 'true', 'crucial', 'white', 'bloodcell', 'types', 'they', 'also', 'had', 'lower', 'levels', 'of', 'antibodies', 'against', 'the', 'covid19', 'spike', 'protein', 'and', 'in', 'some', 'cases', 'higher', 'levels', 'of', 'the', 'spike', 'protein', 'itself', 'we', 'dont', 'know', 'if', 'the', 'level', 'of', 'spike', 'protein', 'is', 'causing', 'the', 'chronic', 'symptoms', 'because', 'there', 'were', 'other', 'participants', 'with', 'pvs', 'postvaccination', 'syndrome', 'who', 'didnt', 'have', 'any', 'measurable', 'spike', 'proteinbut', 'it', 'could', 'be', 'one', 'mechanism', 'underlying', 'this', 'syndrome', 'said', 'cosenior', 'study', 'author', 'akiko', 'iwasaki', 'the', 'researchers', 'believe', 'further', 'studies', 'are', 'needed', 'to', 'confirm', 'the', 'studys', 'findings', 'this', 'work', 'is', 'still', 'in', 'its', 'early', 'stages', 'and', 'we', 'need', 'to', 'validate', 'these', 'findings', 'iwasaki', 'added', 'but', 'this', 'is', 'giving', 'us', 'some', 'hope', 'that', 'there', 'may', 'be', 'something', 'that', 'we', 'can', 'use', 'for', 'diagnosis', 'and', 'treatment', 'of', 'pvs', 'down', 'the', 'road']\n"
          ]
        }
      ],
      "source": [
        "#defining function for tokenization\n",
        "\n",
        "#applying function to the column\n",
        "df['msg_tokenized'] = df['msg_lower'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "print(df['msg_tokenized'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq18b2HwanmU",
        "outputId": "e44394da-2198-4d13-a9b5-0699b2d66b9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "#importing nlp library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "#Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2xFhCdW7K87",
        "outputId": "1f6085bb-6938-41a5-97d3-68f1cc483895"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "# Download stopwords if you haven't already\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize stopwords (use only ONE of these lines)\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "# Add custom stopwords\n",
        "custom_stopwords = [\"'\", '\"', '—', '“', '”', '’', '––', '–', 'said', 'human', 'people', 'health', 'also', 'would', 'could', 'said', '2025', '2024', 'dec', 'feb', 'jan', 'nov', 'oct', 'sept', 'aug', 'july', 'june',\n",
        "                                                                                                'may', 'april', 'march']\n",
        "stopwords = stopwords | set(custom_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48_RV8S0gy9w",
        "outputId": "2e4158ea-b793-47ad-aa00-9fe895c02913"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['new', 'condition', 'called', 'postvaccination', 'syndrome', 'acknowledged', 'yale', 'scientists', 'investigating', 'longterm', 'aftereffects', 'covid19', 'vaccination', 'condition', 'characterised', 'variety', 'troubling', 'symptoms', 'including', 'brain', 'fog', 'insomnia', 'tinnitus', 'heart', 'palpitations', 'resembles', 'condition', 'dubbed', 'long', 'covid', 'symptoms', 'typically', 'develop', 'day', 'two', 'vaccination', 'persist', 'months', 'even', 'years', 'worsening', 'time', 'clear', 'individuals', 'experiencing', 'significant', 'challenges', 'vaccination', 'responsibility', 'scientists', 'clinicians', 'listen', 'experiences', 'rigorously', 'investigate', 'underlying', 'causes', 'seek', 'ways', 'help', 'cosenior', 'author', 'harlan', 'krumholz', 'researchers', 'looked', 'blood', 'samples', '42', 'patients', 'experienced', 'symptoms', 'postvaccination', 'syndrome', '22', 'syndrome', 'found', 'lower', 'levels', 'true', 'crucial', 'white', 'bloodcell', 'types', 'lower', 'levels', 'antibodies', 'covid19', 'spike', 'protein', 'cases', 'higher', 'levels', 'spike', 'protein', 'dont', 'know', 'level', 'spike', 'protein', 'causing', 'chronic', 'symptoms', 'participants', 'pvs', 'postvaccination', 'syndrome', 'didnt', 'measurable', 'spike', 'proteinbut', 'one', 'mechanism', 'underlying', 'syndrome', 'cosenior', 'study', 'author', 'akiko', 'iwasaki', 'researchers', 'believe', 'studies', 'needed', 'confirm', 'studys', 'findings', 'work', 'still', 'early', 'stages', 'need', 'validate', 'findings', 'iwasaki', 'added', 'giving', 'us', 'hope', 'something', 'use', 'diagnosis', 'treatment', 'pvs', 'road']\n"
          ]
        }
      ],
      "source": [
        "# defining functions for removing stopwords\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "df['no_stopwords']= df['msg_tokenized'].apply(lambda x:remove_stopwords(x))\n",
        "print(df['no_stopwords'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NzkX0TnehPUJ"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xtnfhr_hnYm",
        "outputId": "5bea9c1d-cc12-47dc-c56f-2929cb55ac58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['new', 'condit', 'call', 'postvaccin', 'syndrom', 'acknowledg', 'yale', 'scientist', 'investig', 'longterm', 'aftereffect', 'covid19', 'vaccin', 'condit', 'characteris', 'varieti', 'troubl', 'symptom', 'includ', 'brain', 'fog', 'insomnia', 'tinnitu', 'heart', 'palpit', 'resembl', 'condit', 'dub', 'long', 'covid', 'symptom', 'typic', 'develop', 'day', 'two', 'vaccin', 'persist', 'month', 'even', 'year', 'worsen', 'time', 'clear', 'individu', 'experienc', 'signific', 'challeng', 'vaccin', 'respons', 'scientist', 'clinician', 'listen', 'experi', 'rigor', 'investig', 'underli', 'caus', 'seek', 'way', 'help', 'cosenior', 'author', 'harlan', 'krumholz', 'research', 'look', 'blood', 'sampl', '42', 'patient', 'experienc', 'symptom', 'postvaccin', 'syndrom', '22', 'syndrom', 'found', 'lower', 'level', 'true', 'crucial', 'white', 'bloodcel', 'type', 'lower', 'level', 'antibodi', 'covid19', 'spike', 'protein', 'case', 'higher', 'level', 'spike', 'protein', 'dont', 'know', 'level', 'spike', 'protein', 'caus', 'chronic', 'symptom', 'particip', 'pv', 'postvaccin', 'syndrom', 'didnt', 'measur', 'spike', 'proteinbut', 'one', 'mechan', 'underli', 'syndrom', 'cosenior', 'studi', 'author', 'akiko', 'iwasaki', 'research', 'believ', 'studi', 'need', 'confirm', 'studi', 'find', 'work', 'still', 'earli', 'stage', 'need', 'valid', 'find', 'iwasaki', 'ad', 'give', 'us', 'hope', 'someth', 'use', 'diagnosi', 'treatment', 'pv', 'road']\n"
          ]
        }
      ],
      "source": [
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "  stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "  return stem_text\n",
        "\n",
        "# consideration: the results of stemming here is not human readable, but according to chatgpt: However, if your goal is purely text classification (like Naive Bayes or TF-IDF), these changes may not be a problem, as long as your model learns meaningful patterns.\n",
        "df['msg_stemmed'] = df['no_stopwords'].apply(lambda x: stemming(x))\n",
        "print(df['msg_stemmed'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b_t032sji6z",
        "outputId": "65e81364-0204-4d0a-e1d2-63a5b940cf33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voxemsG8jtz8",
        "outputId": "25ee7663-5f6c-48a1-c60e-942b12fa67a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['new', 'condition', 'called', 'postvaccination', 'syndrome', 'acknowledged', 'yale', 'scientist', 'investigating', 'longterm', 'aftereffect', 'covid19', 'vaccination', 'condition', 'characterised', 'variety', 'troubling', 'symptom', 'including', 'brain', 'fog', 'insomnia', 'tinnitus', 'heart', 'palpitation', 'resembles', 'condition', 'dubbed', 'long', 'covid', 'symptom', 'typically', 'develop', 'day', 'two', 'vaccination', 'persist', 'month', 'even', 'year', 'worsening', 'time', 'clear', 'individual', 'experiencing', 'significant', 'challenge', 'vaccination', 'responsibility', 'scientist', 'clinician', 'listen', 'experience', 'rigorously', 'investigate', 'underlying', 'cause', 'seek', 'way', 'help', 'cosenior', 'author', 'harlan', 'krumholz', 'researcher', 'looked', 'blood', 'sample', '42', 'patient', 'experienced', 'symptom', 'postvaccination', 'syndrome', '22', 'syndrome', 'found', 'lower', 'level', 'true', 'crucial', 'white', 'bloodcell', 'type', 'lower', 'level', 'antibody', 'covid19', 'spike', 'protein', 'case', 'higher', 'level', 'spike', 'protein', 'dont', 'know', 'level', 'spike', 'protein', 'causing', 'chronic', 'symptom', 'participant', 'pvs', 'postvaccination', 'syndrome', 'didnt', 'measurable', 'spike', 'proteinbut', 'one', 'mechanism', 'underlying', 'syndrome', 'cosenior', 'study', 'author', 'akiko', 'iwasaki', 'researcher', 'believe', 'study', 'needed', 'confirm', 'study', 'finding', 'work', 'still', 'early', 'stage', 'need', 'validate', 'finding', 'iwasaki', 'added', 'giving', 'u', 'hope', 'something', 'use', 'diagnosis', 'treatment', 'pvs', 'road']\n"
          ]
        }
      ],
      "source": [
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "  return lemm_text\n",
        "\n",
        "df['msg_lemmatized'] = df['no_stopwords'].apply(lambda x:lemmatizer(x))\n",
        "print(df['msg_lemmatized'][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShFtzOJymjJU"
      },
      "source": [
        "Second step we will have is processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Assuming df contains 'content' and 'annotation' columns\n",
        "def prepare_data(df):\n",
        "    # Process content column\n",
        "    X = df['msg_lemmatized'].apply(lambda tokens: \" \".join(tokens) if isinstance(tokens, list) else tokens)\n",
        "    y = df['annotation']\n",
        "\n",
        "    print(X[1])\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Vectorize text data\n",
        "    vectorizer = CountVectorizer()\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "    return X_train_vec, X_test_vec, y_train, y_test, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oMcS6sLNmENm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train_naive_bayes(X_train_vec, y_train):\n",
        "    # Initialize and train Gaussian Naive Bayes classifier\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_model.fit(X_train_vec, y_train)  # No need for .toarray() here\n",
        "    return nb_model\n",
        "\n",
        "def train_svm(X_train_vec, y_train):\n",
        "    # Initialize and train SVM classifier with increased max_iter\n",
        "    svm_model = LinearSVC(random_state=42, max_iter=5000)  # Increase max_iter for convergence\n",
        "    svm_model.fit(X_train_vec, y_train)\n",
        "    return svm_model\n",
        "\n",
        "def train_logistic_regression(X_train_vec, y_train):\n",
        "    # Train Logistic Regression with a specified solver\n",
        "    logistic_model = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)  # Use 'lbfgs' solver\n",
        "    logistic_model.fit(X_train_vec, y_train)\n",
        "    return logistic_model\n",
        "\n",
        "def evaluate_model(model, X_test_vec, y_test, model_name):\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "\n",
        "    # Calculate and print metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'\\n{model_name} Results:')\n",
        "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "r9OUyJHVnD_l"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Main execution with cross-validation and validation split\n",
        "def main(df):\n",
        "    # Prepare data\n",
        "    X_train_vec, X_test_vec, y_train, y_test, vectorizer = prepare_data(df)\n",
        "\n",
        "   # Cross-validation for Naive Bayes\n",
        "    nb_model = train_naive_bayes(X_train_vec, y_train)\n",
        "    nb_cv_scores = cross_val_score(nb_model, X_train_vec, y_train, cv=5, scoring='accuracy')\n",
        "    print(f'Naive Bayes Cross-Validation Accuracy: {nb_cv_scores.mean() * 100:.2f}%')\n",
        "\n",
        "    # Cross-validation for SVM\n",
        "    svm_model = train_svm(X_train_vec, y_train)\n",
        "    svm_cv_scores = cross_val_score(svm_model, X_train_vec, y_train, cv=5, scoring='accuracy')\n",
        "    print(f'SVM Cross-Validation Accuracy: {svm_cv_scores.mean() * 100:.2f}%')\n",
        "\n",
        "    # Logistic regression\n",
        "    logistic_model = train_logistic_regression(X_train_vec, y_train)\n",
        "    logistic_regression_cv_scores = cross_val_score(logistic_model, X_train_vec, y_train, cv=5, scoring='accuracy')\n",
        "    print(f'Logistic Regression Cross-Validation Accuracy: {logistic_regression_cv_scores.mean() * 100:.2f}%')\n",
        "\n",
        "    # Train and evaluate Naive Bayes\n",
        "    nb_predictions = evaluate_model(nb_model, X_test_vec, y_test, \"Naive Bayes\")\n",
        "\n",
        "    # Train and evaluate SVM\n",
        "    svm_predictions = evaluate_model(svm_model, X_test_vec, y_test, \"Support Vector Machine\")\n",
        "\n",
        "    # Train and evaluate logistic regression\n",
        "    logistic_regression_predictions = evaluate_model(logistic_model, X_test_vec, y_test, \"Logistic Regression\")\n",
        "\n",
        "\n",
        "    return {\n",
        "        'naive_bayes': {\n",
        "            'model': nb_model,\n",
        "            'predictions': nb_predictions,\n",
        "            'cv_scores': nb_cv_scores\n",
        "        },\n",
        "        'svm': {\n",
        "            'model': svm_model,\n",
        "            'predictions': svm_predictions,\n",
        "            'cv_scores': svm_cv_scores\n",
        "        },\n",
        "        'logistic_regression': {\n",
        "            'model': logistic_model,\n",
        "            'predictions': logistic_regression_predictions,\n",
        "            'val_accuracy': logistic_regression_cv_scores\n",
        "        },\n",
        "        'vectorizer': vectorizer\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3t3DGsmp8P9",
        "outputId": "67f7a0a9-803e-403b-e193-2f015dd13872"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "new condition called postvaccination syndrome acknowledged yale scientist investigating longterm aftereffect covid19 vaccination condition characterised variety troubling symptom including brain fog insomnia tinnitus heart palpitation resembles condition dubbed long covid symptom typically develop day two vaccination persist month even year worsening time clear individual experiencing significant challenge vaccination responsibility scientist clinician listen experience rigorously investigate underlying cause seek way help cosenior author harlan krumholz researcher looked blood sample 42 patient experienced symptom postvaccination syndrome 22 syndrome found lower level true crucial white bloodcell type lower level antibody covid19 spike protein case higher level spike protein dont know level spike protein causing chronic symptom participant pvs postvaccination syndrome didnt measurable spike proteinbut one mechanism underlying syndrome cosenior study author akiko iwasaki researcher believe study needed confirm study finding work still early stage need validate finding iwasaki added giving u hope something use diagnosis treatment pvs road\n",
            "Naive Bayes Cross-Validation Accuracy: 95.04%\n",
            "SVM Cross-Validation Accuracy: 94.60%\n",
            "Logistic Regression Cross-Validation Accuracy: 96.25%\n",
            "\n",
            "Naive Bayes Results:\n",
            "Accuracy: 92.34%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       276\n",
            "           1       0.92      0.93      0.92       285\n",
            "\n",
            "    accuracy                           0.92       561\n",
            "   macro avg       0.92      0.92      0.92       561\n",
            "weighted avg       0.92      0.92      0.92       561\n",
            "\n",
            "\n",
            "Support Vector Machine Results:\n",
            "Accuracy: 94.12%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94       276\n",
            "           1       0.93      0.95      0.94       285\n",
            "\n",
            "    accuracy                           0.94       561\n",
            "   macro avg       0.94      0.94      0.94       561\n",
            "weighted avg       0.94      0.94      0.94       561\n",
            "\n",
            "\n",
            "Logistic Regression Results:\n",
            "Accuracy: 95.90%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.95      0.96       276\n",
            "           1       0.95      0.97      0.96       285\n",
            "\n",
            "    accuracy                           0.96       561\n",
            "   macro avg       0.96      0.96      0.96       561\n",
            "weighted avg       0.96      0.96      0.96       561\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Assuming df is your DataFrame\n",
        "    results = main(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
