{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\agaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\agaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# For RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link  \\\n",
      "0  https://www.infowars.com/posts/the-wall-is-rea...   \n",
      "1  https://www.infowars.com/posts/post-vaccinatio...   \n",
      "2  https://www.infowars.com/posts/half-of-all-wom...   \n",
      "3  https://www.infowars.com/posts/high-levels-of-...   \n",
      "4  https://www.infowars.com/posts/epidemiologist-...   \n",
      "\n",
      "                                               title  annotation  \\\n",
      "0  The Wall Is Real: Half of All Thirty-Something...           0   \n",
      "1  “Post-Vaccination Syndrome:” New Paper Identif...           0   \n",
      "2  Half of All Women in US Aged 30-35 Experiencin...           0   \n",
      "3  High Levels of Microplastics Found in Lungs of...           0   \n",
      "4  Epidemiologist Reveals New Data Linking Covid ...           0   \n",
      "\n",
      "                                             content  \n",
      "0  It’s a staple of the so-called “manosphere”—th...  \n",
      "1  A new condition called “post-vaccination syndr...  \n",
      "2  Half of all women in their 30s are now reporti...  \n",
      "3  The lungs of birds contain significant quantit...  \n",
      "4  Nicolas Hulscher, an epidemiologist with the M...  \n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "data = r\"C:\\Users\\agaro\\Documents\\GitHub\\Chiron\\algorithm\\dataset1.csv\"\n",
    "df = pd.read_csv(data)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices with missing or empty content: Index([58, 185, 203, 210, 274, 289, 2018, 2019, 2020, 2021], dtype='int64')\n",
      "Total rows: 2801\n"
     ]
    }
   ],
   "source": [
    "# Check for missing or empty values\n",
    "uncleanContents = df[df['content'].isna() | (df['content'].str.strip() == '')].index\n",
    "print(f'Indices with missing or empty content: {uncleanContents}')\n",
    "df['content'].fillna('', inplace=True)\n",
    "print(f\"Total rows: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\agaro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing nlp library\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "#Stop words present in the library\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text example:\n",
      "It’s a staple of the so-called “manosphere”—that glamorous corner of the internet where sex-traffick ...\n",
      "\n",
      "Preprocessed text example:\n",
      "staple socalled manospherethat glamorous corner internet sextrafficker guru rub shoulder pickup arti ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text preprocessing functions\n",
    "def remove_punctuation(text):\n",
    "    punctuationfree = \"\".join([i for i in text if i not in string.punctuation and i not in [\"'\", '\"', '—', '“', '”', '’', '––', '–']])\n",
    "    return punctuationfree\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Initialize stopwords\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    custom_stopwords = [\"'\", '\"', '—', '“', '”', '’', '––', '–', 'said', 'human', 'people', 'health', 'also', 'would', 'could', 'said', '2025', '2024', 'dec', 'feb', 'jan', 'nov', 'oct', 'sept', 'aug', 'july', 'june',\n",
    "                                                                                                    'may', 'april', 'march']\n",
    "    stop_words = stop_words | set(custom_stopwords)\n",
    "        \n",
    "    # Remove stopwords\n",
    "    return \" \".join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join([wordnet_lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# Apply preprocessing steps\n",
    "df['clean_text'] = df['content'].apply(remove_punctuation)\n",
    "df['clean_text'] = df['clean_text'].str.lower()\n",
    "df['clean_text'] = df['clean_text'].apply(remove_stopwords)\n",
    "df['clean_text'] = df['clean_text'].apply(lemmatize_text)\n",
    "\n",
    "# Show preprocessing results\n",
    "print(\"Original text example:\")\n",
    "print(df['content'].iloc[0][:100], \"...\\n\")\n",
    "print(\"Preprocessed text example:\")\n",
    "print(df['clean_text'].iloc[0][:100], \"...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for RNN\n",
    "def prepare_data_for_rnn(df, max_words=10000, max_sequence_length=100):\n",
    "    # Texts and labels\n",
    "    texts = df['clean_text'].values\n",
    "    labels = df['annotation'].values\n",
    "    \n",
    "    # Convert labels to integers if they're not already\n",
    "    if not isinstance(labels[0], (int, np.integer)):\n",
    "        label_mapping = {label: i for i, label in enumerate(set(labels))}\n",
    "        labels = np.array([label_mapping[label] for label in labels])\n",
    "        print(f\"Label mapping: {label_mapping}\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    # Convert text to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "    \n",
    "    # Pad sequences to ensure uniform length\n",
    "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
    "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length)\n",
    "    \n",
    "    # Get vocabulary size (for embedding layer)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Convert labels to categorical for multi-class classification if needed\n",
    "    num_classes = len(np.unique(labels))\n",
    "    if num_classes > 2:\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "        print(f\"Using categorical labels for {num_classes} classes\")\n",
    "    \n",
    "    return X_train_padded, X_test_padded, y_train, y_test, vocab_size, num_classes, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN model\n",
    "def build_rnn_model(vocab_size, embedding_dim=100, max_sequence_length=100, num_classes=2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                        output_dim=embedding_dim, \n",
    "                        input_length=max_sequence_length))\n",
    "    \n",
    "    # Bidirectional LSTM layers\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Summary\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate RNN model\n",
    "def train_evaluate_rnn(X_train, X_test, y_train, y_test, vocab_size, num_classes):\n",
    "    # Model parameters\n",
    "    embedding_dim = 100\n",
    "    max_sequence_length = X_train.shape[1]\n",
    "    \n",
    "    # Build model\n",
    "    model = build_rnn_model(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        max_sequence_length=max_sequence_length,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    \n",
    "    # Callbacks for better training\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        filepath='best_rnn_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Training RNN model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping, model_checkpoint],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nEvaluating RNN model on test data...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    if num_classes == 2:\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "        y_test_flat = y_test  # Already flat for binary\n",
    "    else:\n",
    "        y_pred_prob = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "        y_test_flat = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_flat, y_pred))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RNN classification pipeline...\n",
      "Vocabulary size: 44312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model...\n",
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.5778 - loss: 0.6611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 190ms/step - accuracy: 0.5813 - loss: 0.6584 - val_accuracy: 0.8973 - val_loss: 0.3300\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.9604 - loss: 0.1522"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 164ms/step - accuracy: 0.9606 - loss: 0.1514 - val_accuracy: 0.9286 - val_loss: 0.2138\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 155ms/step - accuracy: 0.9912 - loss: 0.0437 - val_accuracy: 0.9286 - val_loss: 0.2859\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 152ms/step - accuracy: 0.9968 - loss: 0.0152 - val_accuracy: 0.9286 - val_loss: 0.3288\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 156ms/step - accuracy: 0.9996 - loss: 0.0041 - val_accuracy: 0.9286 - val_loss: 0.3610\n",
      "\n",
      "Evaluating RNN model on test data...\n",
      "Test Loss: 0.1900\n",
      "Test Accuracy: 0.9376\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93       276\n",
      "           1       0.91      0.98      0.94       285\n",
      "\n",
      "    accuracy                           0.94       561\n",
      "   macro avg       0.94      0.94      0.94       561\n",
      "weighted avg       0.94      0.94      0.94       561\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 60ms/step - accuracy: 0.7332 - loss: 0.6760 - val_accuracy: 0.8884 - val_loss: 0.4467\n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.9260 - loss: 0.3220 - val_accuracy: 0.9062 - val_loss: 0.2385\n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.9719 - loss: 0.1032 - val_accuracy: 0.9196 - val_loss: 0.2191\n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.9958 - loss: 0.0316 - val_accuracy: 0.9018 - val_loss: 0.2372\n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - accuracy: 0.9964 - loss: 0.0158 - val_accuracy: 0.9241 - val_loss: 0.2903\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9534 - loss: 0.1694\n",
      "\n",
      "Simple LSTM Model Accuracy: 0.9412\n"
     ]
    }
   ],
   "source": [
    "# Compare with simpler model (for benchmarking)\n",
    "def train_simple_model(X_train, X_test, y_train, y_test, vocab_size, num_classes):\n",
    "    # Simpler model with single LSTM layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 50, input_length=X_train.shape[1]))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    \n",
    "    if num_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "    \n",
    "    # Evaluate\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\nSimple LSTM Model Accuracy: {accuracy:.4f}\")\n",
    "    return model\n",
    "\n",
    "# Main execution function\n",
    "def main():\n",
    "    print(\"Starting RNN classification pipeline...\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test, vocab_size, num_classes, tokenizer = prepare_data_for_rnn(df)\n",
    "    \n",
    "    # Train and evaluate RNN model\n",
    "    rnn_model, history = train_evaluate_rnn(X_train, X_test, y_train, y_test, vocab_size, num_classes)\n",
    "    \n",
    "    # Optional: Train simpler model for comparison\n",
    "    simple_model = train_simple_model(X_train, X_test, y_train, y_test, vocab_size, num_classes)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'rnn_model': rnn_model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'history': history,\n",
    "        'simple_model': simple_model\n",
    "    }\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
